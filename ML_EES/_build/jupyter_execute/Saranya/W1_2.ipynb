{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2fe2dc8",
   "metadata": {
    "id": "d2fe2dc8"
   },
   "source": [
    "# Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55ed3c",
   "metadata": {
    "id": "3a55ed3c"
   },
   "source": [
    "This summary will give you a brief introduction to classification and regression tacks in Machine Learning\n",
    "\n",
    "Learning objectives:\n",
    "\n",
    "1. Distinguish classification from regression\n",
    "2. Define a loss/cost function\n",
    "3. Understand how to train a logistic/softmax regression for binary/multiclass classification\n",
    "4. Know how to benchmark a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acade3",
   "metadata": {
    "id": "c5acade3"
   },
   "source": [
    "## Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaef07b",
   "metadata": {
    "id": "7aaef07b"
   },
   "source": [
    "Classification is a supervised learning task where the model learns to classify instances into predefined classes or categories.\n",
    "Binary Classification: Classifying instances into two classes (e.g., spam vs. non-spam emails).\n",
    "Multiclass Classification: Classifying instances into multiple classes (e.g., digit recognition, where each digit is a class).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8794b",
   "metadata": {
    "id": "61c8794b"
   },
   "source": [
    "Here are some practical application of classification in Environmental Sciences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea40032",
   "metadata": {
    "id": "dea40032"
   },
   "source": [
    "> 1. Wildlife Identification: Classification techniques can be used to identify animal species from images or audio recordings, supporting wildlife monitoring projects.\n",
    "> 2. Land Cover Classification: Satellite imagery can be classified into various land cover types, aiding in monitoring land use changes over time.\n",
    "> 3. Invasive Species Detection: Developing models that classify invasive species in images, helping conservationists identify and manage ecological threats.\n",
    "> 4. Water Quality Assessment: Using classification algorithms to determine the quality of water bodies based on factors like chemical concentrations and biological indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95861aa",
   "metadata": {
    "id": "d95861aa"
   },
   "source": [
    "Benchmarking/Evaluating Classification Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df786102",
   "metadata": {
    "id": "df786102"
   },
   "source": [
    "> Confusion Matrix: A table that summarizes the model's performance, showing true positives, true negatives, false positives, and false negatives.\n",
    "> Accuracy: The ratio of correct predictions to the total number of instances.\n",
    "> Precision and Recall: Metrics for assessing the model's performance on positive instances.\n",
    "> F1 Score: The harmonic mean of precision and recall, providing a balanced measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06007d",
   "metadata": {
    "id": "4c06007d"
   },
   "source": [
    "> Training a Binary Classifier: Logistic Regression\n",
    "\n",
    "A popular algorithm for binary classification, providing probabilities for each class.\n",
    "Logistic Regression: Logistic regression is used for binary classification tasks, predicting probabilities for each class.\n",
    "Log Loss (Cross-Entropy): The cost function used to evaluate the performance of logistic regression models.\n",
    "Training Logistic Regression: Iteratively optimize the model's parameters using gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7c175",
   "metadata": {
    "id": "71c7c175"
   },
   "source": [
    "> Multiclass Classification:\n",
    "Multiclass classification, also known as multinomial classification, refers to a classification problem where instances are categorized into three or more distinct classes.\n",
    "Example: Classifying images of animals into categories like \"dog,\" \"cat,\" \"elephant,\" and \"lion.\"\n",
    "\n",
    "> Multilabel Classification:\n",
    "Multilabel classification deals with instances that can belong to multiple classes simultaneously. In other words, an instance can have multiple labels associated with it.\n",
    "Example: Tagging a news article with multiple categories like \"politics,\" \"economy,\" and \"technology\" to capture its diverse content.\n",
    "\n",
    ">Multioutput Classification (or Multioutput Regression):\n",
    "Multioutput classification (or regression) involves predicting multiple output variables simultaneously for each instance. Each output variable can have multiple possible values or classes.\n",
    "Example: Predicting both the color and size of a piece of fruit, where color could be \"red,\" \"green,\" or \"yellow,\" and size could be \"small,\" \"medium,\" or \"large.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c79a2",
   "metadata": {
    "id": "271c79a2"
   },
   "source": [
    "**Cost Function :**\n",
    " Cost Function quantifies how well a machine learning model's predictions align with the actual target values. It measures the discrepancy between the predicted values generated by the model and the true values from the training dataset. The objective of a machine learning algorithm is to minimize this cost function, which essentially means improving the model's accuracy and precision in making predictions.\n",
    "\n",
    "The choice of a cost function depends on the nature of the problem—whether it's a classification, regression, or other type of task—and the desired properties of the model's predictions. Different algorithms and tasks require different types of cost functions.\n",
    "\n",
    "Examples:\n",
    "\n",
    "> Mean Squared Error (MSE): Used in regression tasks, it calculates the average squared difference between predicted and actual values. It penalizes larger errors more heavily.\n",
    "\n",
    "> Log Loss (Cross-Entropy Loss): Commonly used in classification tasks, especially in logistic regression and neural networks. It measures the dissimilarity between predicted probabilities and actual binary class labels.\n",
    "\n",
    "> Absolute Error (L1 Loss): Similar to MSE, but it computes the absolute difference between predicted and actual values. It's less sensitive to outliers compared to MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3310d2",
   "metadata": {
    "id": "ee3310d2"
   },
   "source": [
    "**Training a Multiclass Classifier:**\n",
    "> Support Vector Machines (SVM): Effective for both binary and multiclass classification tasks.\n",
    "\n",
    "> Decision Trees: Tree-like structures used for classification, providing interpretable decision rules.\n",
    "\n",
    "> Random Forests: Ensemble methods that combine multiple decision trees for improved performance.\n",
    "\n",
    "> Softmax Regression (Multinomial Logistic Regression): Softmax regression is used for multiclass classification tasks, predicting probabilities for multiple classes. The Softmax function ensures the predicted probabilities sum up to 1.\n",
    "Cross-Entropy Loss: The cost function used to evaluate softmax regression models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d435b94",
   "metadata": {
    "id": "5d435b94"
   },
   "source": [
    "**Error Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76ff47e",
   "metadata": {
    "id": "e76ff47e"
   },
   "source": [
    "Analyze misclassified instances to gain insights into model weaknesses and potential data issues.\n",
    "Explore the Confusion Matrix: Understand common confusion patterns and how they impact the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3437e94",
   "metadata": {
    "id": "f3437e94"
   },
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a4003",
   "metadata": {
    "id": "e58a4003"
   },
   "source": [
    "### Linear Regression:\n",
    "Linear regression is a supervised machine learning algorithm used for predicting a continuous numerical output based on one or more input features. It assumes a linear relationship between the inputs and the target variable. The goal of linear regression is to find the best-fitting line (or hyperplane in higher dimensions) that minimizes the difference between the predicted values and the actual target values. The most common cost function used in linear regression is the Mean Squared Error (MSE).\n",
    "\n",
    "### Multiple Linear Regression:\n",
    "Multiple linear regression is an extension of linear regression that deals with multiple input features. Instead of just one input, there are multiple independent variables influencing the target variable. The algorithm estimates the coefficients for each feature, determining their individual impact on the target variable while considering their interrelationships.\n",
    "\n",
    "### Other Regression Methods:\n",
    "\n",
    "> Polynomial Regression: This type of regression extends linear regression to capture nonlinear relationships by introducing polynomial terms of the input features. It fits a curve to the data instead of a straight line.\n",
    "\n",
    "> Ridge Regression (L2 Regularization): Ridge regression adds a regularization term to the linear regression cost function. It helps prevent overfitting by penalizing large coefficient values, thus promoting simpler models.\n",
    "\n",
    ">  Lasso Regression (L1 Regularization): Similar to ridge regression, lasso regression also adds a regularization term. However, it uses the absolute values of coefficients, often resulting in some coefficients being exactly zero. This leads to feature selection.\n",
    "\n",
    ">  Elastic Net Regression: Elastic Net combines L1 and L2 regularization to balance the strengths of both. It can handle situations where there are correlated features.\n",
    "\n",
    ">  Support Vector Regression (SVR): SVR applies the principles of support vector machines to regression problems. It aims to fit a hyperplane that captures as many instances within a specified margin as possible.\n",
    "\n",
    ">  Decision Tree Regression: Similar to classification decision trees, decision tree regression predicts a continuous target value by partitioning the feature space into regions and assigning the average target value of instances within each region.\n",
    "\n",
    ">  Random Forest Regression: An ensemble method combining multiple decision tree regressors. It improves predictive accuracy and reduces overfitting by averaging the predictions of individual trees.\n",
    "\n",
    ">  Gradient Boosting Regression: A boosting technique that builds an additive model in a forward stage-wise manner. It combines the predictions of weak learners (often decision trees) to create a strong predictive model.\n",
    "\n",
    "\n",
    "Each regression method has its own strengths, weaknesses, and applicability to different types of data and problem domains. The choice of which method to use depends on the nature of the data, the problem's requirements, and the desired level of interpretability and predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910cb536",
   "metadata": {
    "id": "910cb536"
   },
   "source": [
    "**Gradient Descent: An optimization technique used to minimize the cost function and find the optimal model parameters.**  \n",
    " An optimization technique to minimize the cost function and determine the optimal model parameters. It relies on the partial derivative of the cost function with respect to the model parameters to iteratively improve those parameters. An important parameter of Gradient Descent is the learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SRu-AeIAuNNO",
   "metadata": {
    "id": "SRu-AeIAuNNO"
   },
   "source": [
    "![gradient.png](https://github.com/tbeucler/2023_MLEES_JB/blob/main/ML_EES/Saranya/Figures/gradient.png?raw=1)\n",
    "\n",
    "A suitable learning rate is very important and directly affects the likelihood for the ML model to reach the true minima. A small learning rate will result in a slow convergence, whereas a learning rate that is too large might render the model unable to reach the true minima.\n",
    "\n",
    "Figure credit: An Easy Guide to Gradient Descent in Machine Learning, Great Learning. ([link](https://www.mygreatlearning.com/blog/gradient-descent/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d2e7d",
   "metadata": {
    "id": "077d2e7d"
   },
   "source": [
    "**Overfitting: High-degree polynomials can lead to overfitting the training data.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7651a93",
   "metadata": {
    "id": "b7651a93"
   },
   "source": [
    "The optimization techniques using Gradient Descent include the following approaches:\n",
    "\n",
    ">Batch Gradient Descent: Updates model parameters using the entire training set.\n",
    "\n",
    ">Stochastic Gradient Descent (SGD): Updates parameters based on a single training instance or a small batch of instances.\n",
    "\n",
    ">Mini-batch Gradient Descent: A compromise between batch and SGD, updating parameters using a small batch of instances.\n",
    "\n",
    "| Category  | Advantages | Disadvantages |\n",
    "| ------------- | ------------- | -------------|\n",
    "| Batch Gradient Descent | Stable error gradient  | Requires entire training data in memory |\n",
    "|              | Parallelizable  | Slow model updates and convergence |\n",
    "| Stochastic Gradient Descent  | Avoids stucking in local minimas  | Noisy gradient and large variance during training |\n",
    "|              | Improvements in every model update  | Computationally intensive |\n",
    "| Mini-batch Gradient Descent  | Computationally efficient  | Extra hyperparameter to tune |\n",
    "|              | Smoother learning curves than SGD  | Need to accumulate error over batches |\n",
    "|              | Less memory intensive  | |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m6HCVXjsp85Q",
   "metadata": {
    "id": "m6HCVXjsp85Q"
   },
   "source": [
    "![batch-1.png](https://github.com/tbeucler/2023_MLEES_JB/blob/main/ML_EES/Saranya/Figures/batch-1.png?raw=1)\n",
    "\n",
    "The above figure shows the different ways the three gradient descent methods summarized above approaches the true minima. The SGD can have a very noisy learning process, whereas mini-batch is a compromise between SGD and the smooth batch method.\n",
    "\n",
    "Figure credit: Relation between Learning Rate and Batch Size, Baeldung. ([link](https://www.baeldung.com/cs/learning-rate-batch-size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7cc3c",
   "metadata": {
    "id": "d9d7cc3c"
   },
   "source": [
    "**Fine-Tuning Models:**\n",
    "Learning Rate Scheduling: Adjusting the learning rate during training to converge faster and prevent overshooting.\n",
    "\n",
    "Early Stopping: Stopping training when the model's performance on the validation set no longer improves to prevent overfitting."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}