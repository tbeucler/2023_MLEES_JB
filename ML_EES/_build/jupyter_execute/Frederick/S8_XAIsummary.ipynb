{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tbeucler/2023_MLEES_JB/blob/main/ML_EES/Frederick/S8_XAIsummary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbldirKhyy65"
   },
   "source": [
    "# Explainable AI (XAI)\n",
    "\n",
    "## Learning objectives:\n",
    "1. Understand the importance of XAI\n",
    "2. Distinguish local from global explanation methods\n",
    "3. Distinguish model-agnostic from model-specific methods\n",
    "4. Know how to implement permutation feature importance and partial dependence plots\n",
    "5. Apprehend explanation methods for neural networks.\n",
    "\n",
    "This chapter concerns the steps we can take to make ML model more interpretable or explainable, and introduce some tools we can use to extract physical understanding from data.\n",
    "\n",
    "Broadly speaking, a ML model that is interpretable means that human decision makers understand the cause of a decision and can anticipate the model's predictions. The simplest interpretable model is a simple multiple linear regression (MLR) model.\n",
    "\n",
    "## 1. **When do we need ML models to be explainable?**\n",
    "The first question we need to ask ourselves is \"do we really need a model to the interpretable?\" This question stems from the **trade-offs** between predictive accuracy and model interpretability. A MLR model is often less accurate than complex neural networks for complex problems. Neural networks are too complex to be directly interpretable, however.\n",
    "\n",
    "**Critical decision-making**: The first scenario you would want a model to be interpretable is in the context of high-stakes cases. Medical settings can be one situation where the extra transparency in your models would be very helpful. Can you think of one example where this transparency is not needed?\n",
    "\n",
    "**Human Understanding**: The basic role of a science pratitioner is to discover new knowledge from observations. By analyzing how machine learning models make predictions, we may get some new understanding on, e.g., what differentiates premium wines to cheap ones? It is also possible to use tools to leverage interpretability to extract knowledge embedded in machine learning models (**Extract Additional Instights**)\n",
    "\n",
    "**Increase trust:** AI/ML techniques are relatively new to a lot of different fields, such as weather forecasting. If we can explain how the ML models work in simple terms, forecasters may trust the AL predictions more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KD7vxRLYB-F1"
   },
   "source": [
    "## 2. **Distinguishing Local vs. Global Explanation Methods:**\n",
    "- **Local Explanations:**\n",
    "  - Focus on explaining individual predictions or instances.\n",
    "  - Better accuracy for specific predictions but might not represent the overall model behavior.\n",
    "  - Examples: Counterfactual explanations, instance-based methods.\n",
    "\n",
    "- **Global Explanations:**\n",
    "  - Aimed at understanding the overall model behavior.\n",
    "  - Highlights feature importance and interactions across the entire dataset.\n",
    "  - Examples: Feature importance rankings, aggregated insights.\n",
    "\n",
    "## 3. **Model-Agnostic vs. Model-Specific Methods:**\n",
    "- **Model-Agnostic Methods:**\n",
    "  - Applicable to various machine learning models.\n",
    "  - Post hoc techniques that analyze models after training.\n",
    "  - Examples: Permutation feature importance, SHAP (SHapley Additive exPlanations).\n",
    "\n",
    "- **Model-Specific Methods:**\n",
    "  - Tailored to particular model architectures or types.\n",
    "  - Interpretation based on specific model internals or structures.\n",
    "  - Examples: Interpreting weights in linear models, decision tree visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SseO4iYCKIY7"
   },
   "source": [
    "## 4. **Implementing Permutation Feature Importance and Partial Dependence Plots:**\n",
    "- **Partial Dependence Plots:**\n",
    "  - Visualizing the marginal effect of a feature on the predicted outcome.\n",
    "  - To create these plots, we would first decide an input feature that we are interested in.\n",
    "  - Summarizes the model predictions for different parts of the distribution of the interested input feature.\n",
    "  - Put the statistical summary of predictions for different input feature values to a plot.\n",
    "  - Illustrates how changes in a feature impact model predictions.\n",
    "  - It is useful for regression tasks and classification tasks where the ML model outputs probabilities.\n",
    "  - Useful for understanding complex feature interactions.\n",
    "\n",
    "- **PDP-based Feature Importance:**\n",
    "  - Possible to combine the information in a PDP plot into a single numerical value.\n",
    "  - The numerical value shows how each PDP curve deviates from the average curve for all features.\n",
    "  - One disadvantage is that PDP-based feature importance ignores feature interactions. Only features that are directly impactful are important in this analysis.\n",
    "\n",
    "- **Advantages of PDPs:**\n",
    "  - Intuitive\n",
    "  - Clear interpretation\n",
    "  - A causal interpretation\n",
    "\n",
    "- **Disadvantages of PDPs:**\n",
    "  - Assumption of Independence\n",
    "  - Only shows 2D plots (many figures for a model with a lot of input features)\n",
    "\n",
    "- **Permutation Feature Importance:**\n",
    "  - Shuffling feature values to evaluate their impact on model performance.\n",
    "  - Identifying influential features by measuring performance degradation.\n",
    "  - Practical application in assessing feature relevance.\n",
    "  - Makes more sense to use the test data to calculate the importance, because a feature that is important in an overfitting model can just be unhelpful noise.\n",
    "\n",
    "- **Advantages of Permutation Feature Importances:**\n",
    "  - Compressed, global insight into how the model works\n",
    "  - Clear interpretation\n",
    "  - Accounts for all possible interactions between features\n",
    "  - There are explanation methods that requires you to remove features and retrain models. This method does not require you to do that.\n",
    "\n",
    "\n",
    "- **Disadvantages of Permutation Feature Importances:**\n",
    "  - Linked to the error of the model. Sometimes how predictions change with feature (linear vs. nonlinear) is what we want.\n",
    "  - Does not work when you do not have labels (true outcomes).\n",
    "  - Based on random shuffling. Results can be unstable.\n",
    "  - Importance values can be misleading when there are multicollinearity (features correlate with one another).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGyf7oaTEWH5"
   },
   "source": [
    "## 5. **Explanation Methods for Neural Networks:**\n",
    "For Neural Networks, it is still useful to use model-agnostic methods discussed earlier, it may be even more useful to use interpretation methods designed specifically for NNs.\n",
    "\n",
    "Compared to model-agnostic methods, these methods are useful because:\n",
    "- These methods uncover learned featres and concepts in the hidden layers\n",
    "- Most model-agnostic methods are originally designed for tabular data\n",
    "- Can be more efficient\n",
    "\n",
    "The simplest explanation method for neural networks is **Vanilla Gradient (Saliency Maps)**.\n",
    "- Gradient between machine learning predictions (class score of interest in the case of classification) with respect to the input pixels.\n",
    "- A plot that summarizes the relevance of each pixel in an image to the machine learning model prediction.\n",
    "\n",
    "Other methods include:\n",
    "- **DeconvNet:**\n",
    "  - Reverse a neural network (from prediction to inputs)\n",
    "- **Grad-CAM:**\n",
    "  - Backpropagates the gradient to the last convolutional layer.\n",
    "  - It finds smooth representations of abstract features learned by the CNN model.\n",
    "  - This method gives us *heat maps*\n",
    "- **Guided Grad-CAM**\n",
    "- **SmoothGrad:**\n",
    "  - Add noise and averaging over the artifically noisy gradient.\n",
    "  - This method gives smoother explanations than other gradient-based methods (e.g., Vanilla Gradient)\n",
    "\n",
    "These are **pixle attribution methods** that assigns relevance / importance to every points in the image.\n",
    "\n",
    "Disadvantages of using these methods:\n",
    "- **No ground truth:**\n",
    "  - Different attribution methods often gives very different, even contradictory results.\n",
    "  - Since these explanations are not directly observed, it is difficult to say which method gives the most trustworthy explanation.\n",
    "- **Fragile to noise:**\n",
    "  - Adding adversarial perturbations to an image can lead to very different explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F_dRqGuyJsih"
   },
   "outputs": [],
   "source": [
    "# - **Layer-wise Relevance Propagation (LRP):**\n",
    "#  - Assigning relevance scores to individual neurons or layers.\n",
    "#  - Tracing contributions of input features to model predictions.\n",
    "#  - Enhancing interpretability in neural network decision-making.\n",
    "\n",
    "#- **Activation Maximization:**\n",
    "#  - Generating inputs that maximally activate specific neurons.\n",
    "#  - Understanding what patterns or features trigger certain neural responses.\n",
    "#  - Useful for visualizing what the model \"looks for\" in data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}