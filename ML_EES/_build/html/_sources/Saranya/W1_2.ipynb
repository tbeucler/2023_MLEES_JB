{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2fe2dc8",
   "metadata": {
    "id": "d2fe2dc8"
   },
   "source": [
    "# Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55ed3c",
   "metadata": {
    "id": "3a55ed3c"
   },
   "source": [
    "This summary will give you a brief introduction to classification and regression tacks in Machine Learning\n",
    "\n",
    "Learning objectives:\n",
    "\n",
    "1. Distinguish classification from regression\n",
    "2. Define a loss/cost function\n",
    "3. Understand how to train a logistic/softmax regression for binary/multiclass classification\n",
    "4. Know how to benchmark a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acade3",
   "metadata": {
    "id": "c5acade3"
   },
   "source": [
    "## Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaef07b",
   "metadata": {
    "id": "7aaef07b"
   },
   "source": [
    "Classification is a supervised learning task where the model learns to classify instances into predefined classes or categories.\n",
    "Binary Classification: Classifying instances into two classes (e.g., spam vs. non-spam emails).\n",
    "Multiclass Classification: Classifying instances into multiple classes (e.g., digit recognition, where each digit is a class).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8794b",
   "metadata": {
    "id": "61c8794b"
   },
   "source": [
    "Here are some practical application of classification in Environmental Sciences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea40032",
   "metadata": {
    "id": "dea40032"
   },
   "source": [
    "> 1. Wildlife Identification: Classification techniques can be used to identify animal species from images or audio recordings, supporting wildlife monitoring projects.\n",
    "> 2. Land Cover Classification: Satellite imagery can be classified into various land cover types, aiding in monitoring land use changes over time.\n",
    "> 3. Invasive Species Detection: Developing models that classify invasive species in images, helping conservationists identify and manage ecological threats.\n",
    "> 4. Water Quality Assessment: Using classification algorithms to determine the quality of water bodies based on factors like chemical concentrations and biological indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95861aa",
   "metadata": {
    "id": "d95861aa"
   },
   "source": [
    "In Machine Learning, it is critical to make sure that your model is well trained and can generalize well to unseen data during training. For this purpose, we rely on different performance metrics as quantifiable measures for model skills.\n",
    "\n",
    "The most simple skill score for classification models is the *Accuracy Score*, which is the ratio between correct predictions and the total number of instances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_PqVaCu0yDSG",
   "metadata": {
    "id": "_PqVaCu0yDSG"
   },
   "source": [
    "**The balancing act between precision and recall :**\n",
    "\n",
    "The accuracy score is intuitive and straight-forward, but using it to benchmark your classification models can be problematic for real data, particularly if you are dealing with unbalanced data.\n",
    "\n",
    "Assuming that 90\\% of your data belongs to the positive class, then the model can have no skills on preddicting negative class and still have a good precision score. It is definitely not ideal that your classification model can only give you one answer no matter how your samples look like!\n",
    "\n",
    "Fortunately, we have skill scores that are useful for unbalanced data. These scores balance the precision and the recall of a model.\n",
    "\n",
    "> Precision: The ratio between correctly-predicted positives (True Positive; TP) and the number of instances the model predicted positive (True Positive + False Positive). \\\\\n",
    "> Recall: The ratio between TP and the number of real positives in the data (True Positive + False Negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JIP1fuBVwwqV",
   "metadata": {
    "id": "JIP1fuBVwwqV"
   },
   "source": [
    "We can use a Confusion Matrix table to summary these information in an attractive visual format.\n",
    "\n",
    "| |  |\n",
    "|---------- |-------------  |\n",
    "| True Positive | False Positive  |\n",
    "| False Negative| True Negative  |\n",
    "\n",
    "Scikit-learn provides a function to access the confusion matrix: \\\\\n",
    "sklearn.metrics.confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df786102",
   "metadata": {
    "id": "df786102"
   },
   "source": [
    "The other metric that balances precision and recall is the F1 Score.\n",
    "\n",
    "> F1 Score: The harmonic mean of precision and recall, providing a balanced measure. \\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06007d",
   "metadata": {
    "id": "4c06007d"
   },
   "source": [
    "> Training a Binary Classifier: Logistic Regression\n",
    "\n",
    "A popular algorithm for binary classification, providing probabilities for each class.\n",
    "Logistic Regression: Logistic regression is used for binary classification tasks, predicting probabilities for each class.\n",
    "Log Loss (Cross-Entropy): The cost function used to evaluate the performance of logistic regression models.\n",
    "Training Logistic Regression: Iteratively optimize the model's parameters using gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7c175",
   "metadata": {
    "id": "71c7c175"
   },
   "source": [
    "> Multiclass Classification:\n",
    "Multiclass classification, also known as multinomial classification, refers to a classification problem where instances are categorized into three or more distinct classes.\n",
    "Example: Classifying images of animals into categories like \"dog,\" \"cat,\" \"elephant,\" and \"lion.\"\n",
    "\n",
    "> Multilabel Classification:\n",
    "Multilabel classification deals with instances that can belong to multiple classes simultaneously. In other words, an instance can have multiple labels associated with it.\n",
    "Example: Tagging a news article with multiple categories like \"politics,\" \"economy,\" and \"technology\" to capture its diverse content.\n",
    "\n",
    ">Multioutput Classification (or Multioutput Regression):\n",
    "Multioutput classification (or regression) involves predicting multiple output variables simultaneously for each instance. Each output variable can have multiple possible values or classes.\n",
    "Example: Predicting both the color and size of a piece of fruit, where color could be \"red,\" \"green,\" or \"yellow,\" and size could be \"small,\" \"medium,\" or \"large.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c79a2",
   "metadata": {
    "id": "271c79a2"
   },
   "source": [
    "**Cost Function :**\n",
    " Cost Function quantifies how well a machine learning model's predictions align with the actual target values. It measures the discrepancy between the predicted values generated by the model and the true values from the training dataset. The objective of a machine learning algorithm is to minimize this cost function, which essentially means improving the model's accuracy and precision in making predictions.\n",
    "\n",
    "The choice of a cost function depends on the nature of the problem—whether it's a classification, regression, or other type of task—and the desired properties of the model's predictions. Different algorithms and tasks require different types of cost functions.\n",
    "\n",
    "Examples:\n",
    "\n",
    "> Mean Squared Error (MSE): Used in regression tasks, it calculates the average squared difference between predicted and actual values. It penalizes larger errors more heavily.\n",
    "\n",
    "> Log Loss (Cross-Entropy Loss): Commonly used in classification tasks, especially in logistic regression and neural networks. It measures the dissimilarity between predicted probabilities and actual binary class labels.\n",
    "\n",
    "> Absolute Error (L1 Loss): Similar to MSE, but it computes the absolute difference between predicted and actual values. It's less sensitive to outliers compared to MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3310d2",
   "metadata": {
    "id": "ee3310d2"
   },
   "source": [
    "**Training a Multiclass Classifier:**\n",
    "> Support Vector Machines (SVM): Effective for both binary and multiclass classification tasks.\n",
    "\n",
    "> Decision Trees: Tree-like structures used for classification, providing interpretable decision rules.\n",
    "\n",
    "> Random Forests: Ensemble methods that combine multiple decision trees for improved performance.\n",
    "\n",
    "> Softmax Regression (Multinomial Logistic Regression): Softmax regression is used for multiclass classification tasks, predicting probabilities for multiple classes. The Softmax function ensures the predicted probabilities sum up to 1.\n",
    "Cross-Entropy Loss: The cost function used to evaluate softmax regression models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d435b94",
   "metadata": {
    "id": "5d435b94"
   },
   "source": [
    "**Error Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76ff47e",
   "metadata": {
    "id": "e76ff47e"
   },
   "source": [
    "Analyze misclassified instances to gain insights into model weaknesses and potential data issues.\n",
    "Explore the Confusion Matrix: Understand common confusion patterns and how they impact the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3437e94",
   "metadata": {
    "id": "f3437e94"
   },
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a4003",
   "metadata": {
    "id": "e58a4003"
   },
   "source": [
    "### Linear Regression:\n",
    "Linear regression is a supervised machine learning algorithm used for predicting a continuous numerical output based on one or more input features. It assumes a linear relationship between the inputs and the target variable. The goal of linear regression is to find the best-fitting line (or hyperplane in higher dimensions) that minimizes the difference between the predicted values and the actual target values. The most common cost function used in linear regression is the Mean Squared Error (MSE).\n",
    "\n",
    "### Multiple Linear Regression:\n",
    "Multiple linear regression is an extension of linear regression that deals with multiple input features. Instead of just one input, there are multiple independent variables influencing the target variable. The algorithm estimates the coefficients for each feature, determining their individual impact on the target variable while considering their interrelationships.\n",
    "\n",
    "### Other Regression Methods:\n",
    "\n",
    "> Polynomial Regression: This type of regression extends linear regression to capture nonlinear relationships by introducing polynomial terms of the input features. It fits a curve to the data instead of a straight line.\n",
    "\n",
    "> Ridge Regression (L2 Regularization): Ridge regression adds a regularization term to the linear regression cost function. It helps prevent overfitting by penalizing large coefficient values, thus promoting simpler models.\n",
    "\n",
    ">  Lasso Regression (L1 Regularization): Similar to ridge regression, lasso regression also adds a regularization term. However, it uses the absolute values of coefficients, often resulting in some coefficients being exactly zero. This leads to feature selection.\n",
    "\n",
    ">  Elastic Net Regression: Elastic Net combines L1 and L2 regularization to balance the strengths of both. It can handle situations where there are correlated features.\n",
    "\n",
    ">  Support Vector Regression (SVR): SVR applies the principles of support vector machines to regression problems. It aims to fit a hyperplane that captures as many instances within a specified margin as possible.\n",
    "\n",
    ">  Decision Tree Regression: Similar to classification decision trees, decision tree regression predicts a continuous target value by partitioning the feature space into regions and assigning the average target value of instances within each region.\n",
    "\n",
    ">  Random Forest Regression: An ensemble method combining multiple decision tree regressors. It improves predictive accuracy and reduces overfitting by averaging the predictions of individual trees.\n",
    "\n",
    ">  Gradient Boosting Regression: A boosting technique that builds an additive model in a forward stage-wise manner. It combines the predictions of weak learners (often decision trees) to create a strong predictive model.\n",
    "\n",
    "\n",
    "Each regression method has its own strengths, weaknesses, and applicability to different types of data and problem domains. The choice of which method to use depends on the nature of the data, the problem's requirements, and the desired level of interpretability and predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d163dfce",
   "metadata": {},
   "source": [
    "**Normal Equation vs Gradient Descent**\n",
    "\n",
    ">The normal equation provides a closed-form solution to find the optimal parameters that minimize the cost function in linear regression. It is particularly useful when dealing with small to moderately sized datasets as it efficiently computes the optimal parameters directly. On the other hand, gradient descent is an iterative optimization algorithm that works well with large datasets and complex models. It iteratively updates model parameters in the direction of steepest descent, making it suitable for high-dimensional spaces and non-linear models. The two methods complement each other in machine learning. While the normal equation offers a straightforward analytical solution for simple cases, gradient descent shines in handling more complex scenarios, where computational efficiency and adaptability to various models and data sizes are paramount. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910cb536",
   "metadata": {
    "id": "910cb536"
   },
   "source": [
    "**Gradient Descent: An optimization technique used to minimize the cost function and find the optimal model parameters.**  \n",
    " An optimization technique to minimize the cost function and determine the optimal model parameters. It relies on the partial derivative of the cost function with respect to the model parameters to iteratively improve those parameters. An important parameter of Gradient Descent is the learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SRu-AeIAuNNO",
   "metadata": {
    "id": "SRu-AeIAuNNO"
   },
   "source": [
    "![gradient.png](https://github.com/tbeucler/2023_MLEES_JB/blob/main/ML_EES/Saranya/Figures/gradient.png?raw=1)\n",
    "\n",
    "A suitable learning rate is very important and directly affects the likelihood for the ML model to reach the true minima. A small learning rate will result in a slow convergence, whereas a learning rate that is too large might render the model unable to reach the true minima.\n",
    "\n",
    "Figure credit: An Easy Guide to Gradient Descent in Machine Learning, Great Learning. ([link](https://www.mygreatlearning.com/blog/gradient-descent/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d2e7d",
   "metadata": {
    "id": "077d2e7d"
   },
   "source": [
    "**Overfitting: High-degree polynomials can lead to overfitting the training data.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7651a93",
   "metadata": {
    "id": "b7651a93"
   },
   "source": [
    "The optimization techniques using Gradient Descent include the following approaches:\n",
    "\n",
    ">Batch Gradient Descent: Updates model parameters using the entire training set.\n",
    "\n",
    ">Stochastic Gradient Descent (SGD): Updates parameters based on a single training instance or a small batch of instances.\n",
    "\n",
    ">Mini-batch Gradient Descent: A compromise between batch and SGD, updating parameters using a small batch of instances.\n",
    "\n",
    "| Category  | Advantages | Disadvantages |\n",
    "| ------------- | ------------- | -------------|\n",
    "| Batch Gradient Descent | Stable error gradient  | Requires entire training data in memory |\n",
    "|              | Parallelizable  | Slow model updates and convergence |\n",
    "| Stochastic Gradient Descent  | Avoids stucking in local minimas  | Noisy gradient and large variance during training |\n",
    "|              | Improvements in every model update  | Computationally intensive |\n",
    "| Mini-batch Gradient Descent  | Computationally efficient  | Extra hyperparameter to tune |\n",
    "|              | Smoother learning curves than SGD  | Need to accumulate error over batches |\n",
    "|              | Less memory intensive  | |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m6HCVXjsp85Q",
   "metadata": {
    "id": "m6HCVXjsp85Q"
   },
   "source": [
    "![batch-1.png](https://github.com/tbeucler/2023_MLEES_JB/blob/main/ML_EES/Saranya/Figures/batch-1.png?raw=1)\n",
    "\n",
    "The above figure shows the different ways the three gradient descent methods summarized above approaches the true minima. The SGD can have a very noisy learning process, whereas mini-batch is a compromise between SGD and the smooth batch method.\n",
    "\n",
    "Figure credit: Relation between Learning Rate and Batch Size, Baeldung. ([link](https://www.baeldung.com/cs/learning-rate-batch-size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7cc3c",
   "metadata": {
    "id": "d9d7cc3c"
   },
   "source": [
    "**Fine-Tuning Models:**\n",
    "Learning Rate Scheduling: Adjusting the learning rate during training to converge faster and prevent overshooting.\n",
    "\n",
    "Early Stopping: Stopping training when the model's performance on the validation set no longer improves to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bf3ec",
   "metadata": {},
   "source": [
    "**Learning Rate** is a small positive scalar value that controls the step size at each iteration during model training. It scales the gradient (derivative) of the loss function with respect to the model parameters.\n",
    "\n",
    "Impact on Training: A high learning rate can lead to rapid convergence but risks overshooting the optimal solution, potentially causing the model to diverge. Conversely, a low learning rate can lead to slow convergence and might get stuck in local minima.\n",
    "\n",
    "Learning rate is a crucial hyperparameter in machine learning algorithms, especially those that involve optimization techniques like gradient descent. It determines the size of the steps taken when updating model parameters during the training process. Understanding and setting an appropriate learning rate is vital, as it can significantly impact the training convergence, model performance, and the ability to find the optimal solution. In environmental science, where machine learning is applied to various prediction and forecasting tasks, choosing an appropriate learning rate is essential for achieving accurate and efficient models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809c131",
   "metadata": {},
   "source": [
    "Some examples:\n",
    "\n",
    "> Air Quality Forecasting: In predicting air quality levels, machine learning models may require careful tuning of the learning rate. An appropriately chosen learning rate can help the model converge to an accurate representation of air quality based on historical data.\n",
    "\n",
    "> Climate Modeling: Climate models often involve complex optimization problems. Setting the learning rate correctly can accelerate the training process and enable better understanding of climate patterns.\n",
    "\n",
    "> Hydrological Forecasting: When predicting river flows or flood levels, an optimal learning rate can ensure that hydrological models converge to accurate predictions, improving early warning systems.\n",
    "\n",
    "> Land Cover Classification: In tasks like land cover classification using remote sensing data, a well-tuned learning rate can help neural networks efficiently learn the intricate relationships between spectral bands and land cover types.\n",
    "\n",
    "> Ecosystem Modeling: Learning rates are essential in ecosystem models, where they affect the speed and stability of parameter estimation, allowing researchers to gain insights into ecosystem dynamics."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
