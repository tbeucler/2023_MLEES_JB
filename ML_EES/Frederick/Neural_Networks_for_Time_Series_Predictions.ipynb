{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks for Time Series Predictions\n",
        "\n",
        "This week, we will introduce ML approaches for time series data. Time series data documents the temporal changes of variables and is very common when dealing with instrumental data. This chapter will introduce the use of Recurrent Neural Networks and 1D Convolutional Neural Networks to predict future changes in variables with time.\n",
        "\n",
        "Learning Objectives for this chapter include:\n",
        "\n",
        "\n",
        "*   Define a recurrent neural network\n",
        "*   Distinguish recurrent from convolutional neural networks\n",
        "*   Discuss the forecasting of environmental time series\n",
        "*   Define natural language processing\n",
        "*   Know at least three algorithms to process time-series\n",
        "\n"
      ],
      "metadata": {
        "id": "XtPXD_OnqWwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Sequences using RNNs and CNNs"
      ],
      "metadata": {
        "id": "_rjXUBExyJJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![RNN_schematic.jpg](./Figures/RNN_schematic.jpg)"
      ],
      "metadata": {
        "id": "nb-RV20Hxxto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Caption** LSTM Model for time series prediction for environmental sciences\n",
        "\n",
        "**Source** Figure 1 from Mishra et al. (2020; [link to paper](https://www.sciencedirect.com/science/article/pii/S2352484719308546#fig1))"
      ],
      "metadata": {
        "id": "X0Hz-Lt1r3nJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key points**\n",
        "\n",
        "*   Time series forecasting in environmental sciences can be framed in different ways: *Sequence-to-sequence*, *sequence-to-vector*, *vector-to-sequence*\n",
        "\n",
        "*   The *encoder-decoder* structure is essentially a sequence-to-vector model followed by a vector-to-sequence model.\n",
        "\n",
        "*   A recurrent node unrolling through time => the most straightforward RNN architecture\n",
        "\n",
        "*   Stacking multiple layers of recurrent nodes gives you *deep RNNs*, which may have better skills than simple RNNs\n",
        "\n",
        "*   Vanilla RNNs may encounter problems when dealing with long-time series data, such as *unstable gradients* and *short-term memory* problems.\n",
        "\n",
        "*   Using the *Long Short-Term Memory (LSTM)* layers or *Gated Recurrent Units (GRU)* layers in your architecture helps to alleviate the above problems.\n",
        "\n",
        "*   These solutions split the available information into long-term and short-term components and learn which part of long-term information to include in the prediction during training.\n",
        "\n",
        "*   Time series predictions can also be produced with 1D convolutional networks.\n"
      ],
      "metadata": {
        "id": "5YoW1oIM0aAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Environmental Sciences Applications**\n",
        "\n",
        "The neural network architectures introduced in this chapter can be useful when your problem involves predicting the time evolution of different physical variables. One example is using LSTM to predict river streamflow in the western US (e.g., Hunt et al. 2022).\n",
        "\n",
        "Assuming you have a large gridded dataset with time evolution of environmental properties (e.g., topography, vegetation, weather conditions, rainfall predictions), and your research task is to evaluate the likelihood of flooding in the next 12 hours, you can use RNN or similar models to predict the changes in water levels at particular measuring sites. These models make predictions using environmental context from the gridded data and water level predictions at the previous time step.\n",
        "\n",
        "Reference:\n",
        "Hunt, K. M. R., Matthews, G. R., Pappenberger, F., & Prudhomme, C. (2022). Using a long short-term memory (LSTM) neural network to boost river streamflow forecasts over the western united states. Hydrology and Earth System Sciences, 26(21), 5449-5472.\n",
        "([Link to Paper](https://hess.copernicus.org/articles/26/5449/2022/hess-26-5449-2022.html))"
      ],
      "metadata": {
        "id": "UYqcjLdE1CLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise: Composing Music with RNNs / 1D CNNs**\n",
        "\n",
        "The first exercise of this chapter is to use RNNs and 1D CNNs to create new Chorales in the style of Bach. Can you tune the model hyperparameters to create a track that is listenable?\n",
        "\n",
        "1.   Load and preprocess a dataset storing multiple existing Bach chorales\n",
        "2.   Train a small WaveNet to create new chorale in time series format\n",
        "3.   Evaluate model skills\n",
        "4.   Repeat the prediction task with RNNs\n"
      ],
      "metadata": {
        "id": "jgCF6VBCf0YS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers and Attention"
      ],
      "metadata": {
        "id": "-9vmG77EjgUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key points**\n",
        "\n",
        "*   The main disavantages of the RNNs introduced in the previous section is that RNNs have limited capacities to capture long-range dependencies and can be hard to interpret.\n",
        "\n",
        "*   Attention mechanism is a useful technique to improve upon the traditional RNNs.\n",
        "\n",
        "*   The attention layer consists of a encoding part, a decoding part, and a small neural network (alignment model) to find the similarity between different subsections of inputs and outputs.\n",
        "\n",
        "*   The similarity measurements in the model hidden state are interpretable and reveal information on which part of the inputs influences the model prediction the most.\n",
        "\n",
        "*   Attention architecture can be used to process visual images. Visual attention informs which part of a picture influences model predictions.\n",
        "\n",
        "*   The main advantages of incoporating attention mechanism are (a) ability to capture long-range dependencies, (b) interpretable, and (c) potentially improve model prediction skills.\n",
        "\n",
        "*   The attention layer is the vital part of a novel architecture proved to be useful for time series tasks - Transformers.\n",
        "\n",
        "*   Novel aspects of transformers include positional encoding and multi-head attention.\n",
        "\n",
        "*   Using Natural Language Processing tasks as an example, multihead attention trains different iterations of self-attention that captures different long-range dependencies within a sentence.\n",
        "\n",
        "* The multihead attention module will then combine different similarity matrices and provide the final attention output from 0 to 1."
      ],
      "metadata": {
        "id": "rsBl9r3Ohhnf"
      }
    }
  ]
}