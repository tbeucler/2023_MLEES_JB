{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlq0KU9bRoBbQNYvgnkn4j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbeucler/2023_MLEES_JB/blob/main/ML_EES/Frederick/S8_XAIsummary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explainable AI (XAI)\n",
        "\n",
        "## Learning objectives:\n",
        "1. Understand the importance of XAI\n",
        "2. Distinguish local from global explanation methods\n",
        "3. Distinguish model-agnostic from model-specific methods\n",
        "4. Know how to implement permutation feature importance and partial dependence plots\n",
        "5. Apprehend explanation methods for neural networks.\n",
        "\n",
        "## 1. **Importance of XAI:**\n",
        "- **Trade-offs:** Balancing predictive accuracy and model interpretability in decision-making.\n",
        "- **Critical Decision Context:** Necessity for explanations in high-stakes scenarios, safety measures, and addressing incomplete problem formulations.\n",
        "- **Human Understanding:** Facilitating learning, satisfying curiosity, and the human need for comprehensibility.\n",
        "- **Extracting Additional Insights:** Leveraging interpretability to extract additional knowledge embedded in machine learning models.\n",
        "- **Social Acceptance:** Enhancing trust, acceptance, and managing social interactions through understandable AI decisions.\n",
        "\n",
        "## 2. **Distinguishing Local vs. Global Explanation Methods:**\n",
        "- **Local Explanations:**\n",
        "  - Focus on explaining individual predictions or instances.\n",
        "  - Better accuracy for specific predictions but might not represent the overall model behavior.\n",
        "  - Examples: Counterfactual explanations, instance-based methods.\n",
        "\n",
        "- **Global Explanations:**\n",
        "  - Aimed at understanding the overall model behavior.\n",
        "  - Highlights feature importance and interactions across the entire dataset.\n",
        "  - Examples: Feature importance rankings, aggregated insights.\n",
        "\n",
        "## 3. **Model-Agnostic vs. Model-Specific Methods:**\n",
        "- **Model-Agnostic Methods:**\n",
        "  - Applicable to various machine learning models.\n",
        "  - Post hoc techniques that analyze models after training.\n",
        "  - Examples: Permutation feature importance, SHAP (SHapley Additive exPlanations).\n",
        "\n",
        "- **Model-Specific Methods:**\n",
        "  - Tailored to particular model architectures or types.\n",
        "  - Interpretation based on specific model internals or structures.\n",
        "  - Examples: Interpreting weights in linear models, decision tree visualization.\n",
        "\n",
        "## 4. **Implementing Permutation Feature Importance and Partial Dependence Plots:**\n",
        "- **Permutation Feature Importance:**\n",
        "  - Shuffling feature values to evaluate their impact on model performance.\n",
        "  - Identifying influential features by measuring performance degradation.\n",
        "  - Practical application in assessing feature relevance.\n",
        "\n",
        "- **Partial Dependence Plots:**\n",
        "  - Visualizing the marginal effect of a feature on the predicted outcome.\n",
        "  - Illustrating how changes in a feature impact model predictions.\n",
        "  - Useful for understanding complex feature interactions.\n",
        "\n",
        "## 5. **Explanation Methods for Neural Networks:**\n",
        "- **Layer-wise Relevance Propagation (LRP):**\n",
        "  - Assigning relevance scores to individual neurons or layers.\n",
        "  - Tracing contributions of input features to model predictions.\n",
        "  - Enhancing interpretability in neural network decision-making.\n",
        "\n",
        "- **Activation Maximization:**\n",
        "  - Generating inputs that maximally activate specific neurons.\n",
        "  - Understanding what patterns or features trigger certain neural responses.\n",
        "  - Useful for visualizing what the model \"looks for\" in data.\n",
        "\n"
      ],
      "metadata": {
        "id": "UbldirKhyy65"
      }
    }
  ]
}